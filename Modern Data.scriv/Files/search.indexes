<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<SearchIndexes Version="1.0">
    <Documents>
        <Document ID="3">
            <Title>Modern Data</Title>
            <Text>Modern Data is a compact, binary, dependently-typed, self-describing data format for object graphs, and also a library.

A self-describing data format is a format like XML, JSON, or Common Lisp’s s-expressions, which can be processed at a superficial level without knowledge of the specific type of data it contains.  The typical purpose of such formats is interchange across diverse software tools; for example, many tools for working with databases can produce output or accept input in XML.  They can also be useful intermediate representations for transformations on data structures, as with XSLT.  They can even be used for network protocols, as with XMPP/Jabber.  Modern Data is at least as suitable for these tasks as any existing format.

Unlike the above-mentioned alternatives, Modern Data is exclusively a binary format, not a textual one.  This is because textual formats can be written directly by humans, who can make mistakes, which software tools must then cope with, which can be quite difficult and lead to unexpected behavior and, when processing is done inconsistently by different implementations, even to security holes.  A binary format can be written only by the use of assistive software which understands the format, so it is not subject to this problem.

Although not the primary motivation for the choice, a benefit of being a binary format is a certain compactness.  Care has been taken to ensure that the format can in principle be used by database engines and the like as a native format - perhaps slightly larger than non-self-describing forms, but not enormously so as a textual format would be, and without needing a parse phase to perform simple actions such as scanning through it.  Specifically, the format does not use offset-based pointers, so it is suitable for embedding in other formats that do their own space allocation.

It would be difficult to discuss the specifics of Modern Data with no notation for it at all; therefore, there are in fact textual notations for it.  But parsers for these notations are not part of the library - only generators.  This should hopefully remove the temptation to use them for any purpose but documentation and standardization.

The above benefits are substantial, but the most distinctive attribute of Modern Data compared to existing formats is that it is dependently typed.  In a formal sense, this means that types and values are intermixable.  Thus the schema for a Modern Data document is itself potentially a Modern Data document, using a type as a value.  Furthermore, bounds-checked arrays, dimensioned quantities, and the like can all be expressed through values used within types, in ways that will be familiar to programmers who have used theorem-proving languages such as Agda, Coq, Epigram, and Idris.

Another benefit that is a prerequisite of dependent typing but will not be obvious to programmers who are unfamiliar with Hindley-Milner-style type systems is that types are parametric; this means, for example, that it is possible to write the incomplete type “balanced binary tree containing leaf nodes of some particular type”, without specifying the content type at the time that the tree type is defined; the type parameter can be filled in to instantiate that tree for any desired content type, thus obtaining a complete type, which can actually be instantiated.  This will be familiar to Haskell and ML programmers; it is also reminiscent of C++ and Java templates, but the meaning of it has been more rigorously formalized.

Furthermore, code can be written using the library which operates on a parametric type such as our binary tree example, generically to its specific parameters - thus code can be written which rebalances a tree after an update, without knowing the type of the contents.  The library is written such that these operations can be efficiently performed without the need to convert Modern Data to a separate in-memory representation.

Modern Data, unlike other self-describing formats, is capable of representing an arbitrary object graph - not merely one that has obvious linear structure.  This is accomplished without pointers or globally-scoped identifiers of any kind, and therefore does not impede use embedded within other formats, as mentioned above.  The benefit is that arbitrary data structures can be represented directly, including ones which would have cyclic pointers in their traditional in-memory representations.

The Modern Data base library is written in C.  Not because it’s more suitable for the task than other languages would be - it’s not.  Rather, functional languages tend to have runtime systems (largely consisting of garbage collectors) which make the scenario of deploying a library to be linked against from other languages more difficult.  Obviously there will be bindings written in functional languages, as functional programming is the paradigm best equipped to take advantage of Modern Data’s features, but the library will always be in a system language, so that it can easily be linked against from as many other languages as possible.  Care has been taken to play nicely with foreign-function interfaces, which cannot always use the full capabilities of the C ABI.

Please, come on in - and enjoy the type system!</Text>
        </Document>
        <Document ID="4">
            <Title>Why is it written in C?</Title>
        </Document>
        <Document ID="5">
            <Title>Scrawlings</Title>
            <Text>For C, everything is effectively like Data.Dynamic.  For Haskell there will be a GADT and an existential.

Node types “lambda”, “apply”, “index” (as in De Bruijn), and “family” (as in inductive type family).

Things like void (*modern_stream_type_definition_int8)(void *processor, void *context) are callbacks invoked when type IDs are read.  All the functions in that struct are callbacks that are invoked on things that happen during streaming parsing.  The API for streaming generation is the same - except the client code is the consumer of the struct and the caller of these functions, rather than the provider of them.  Note that using the streaming API it is possible to construct invalid output; if this is a concern the object-model API should be used instead.

To make things with cycles, what happens behind the scenes is…  Well, it’s not behind the scenes when you’re doing streaming mode, but in object-model mode, it uses lambdas and applies.  The “prelude” has the S and K combinators already defined, which isn’t strictly necessary because you could define them yourself, but it’s a convenience.

The Lisp-like syntax is the debug format, for use in, say, standards documents that describe APIs in which the units of data interchange are values in Modern Data.

So a point I’m not totally sure on is that I think I just need one version of lambda and one version of apply - as opposed to one of each for values and another of each for types.

Recursive types, such as the simple tree type (in Haskell notation) data Tree = Internal (Tree a) (Tree a) | Leaf a, are defined as cyclic structures.  This avoids the need to refer to anything by name, ever, although the system does in fact support names.

This is on the type level but it’s dependent, so the machinery is the same for types as for values.

I am unclear on whether to implement the set hierarchy, or type-in-type.  I know that I want to call it “universe”, because “set” is a poor name suggestive of set theory rather than type theory.  If I do use the set hierarchy, I expect to have a finite limit, probably 64 bits, on how high one can go.  This is not really any more of a wart than a limit placed on how large an array can be.

With both lambdas and applies present, in principle full computation is possible.

Some parts of this are simply matters of good C practice than of dependent types.  For example, the use of vfile structures for when you want to read a Modern Data value that’s in a zipfile.

You can basically use Modern Data for anything you presently do with XML or HTML or JSON or whatnot, except you can also communicate types across it.  So schemas are built in.  XML schema description is a bugbear that they’ve had a lot of failed attempts at.  You can “teach” the other end how to at least process your thing, even if it doesn’t know your types in advance, by sending a type-as-data across the wire.  You can write helper utilities that only understand some of the types and not the rest, to make “map” and other higher-order functions.

The original motivator was because I hate using text as the format for shell commands.

There should be all sorts of applications.  I think it applies to the functional-database problem, too, even.

In principle you could write webpages in it.

You could implement a programming “language” in it.  The source code would be simply a data structure, instead of being text.  If you like text, you can treat the textual presentation as simply a formatting layer.

Once the basic library is working I’m going to make a generic graphical editor for this, for the Mac.

You can use it for network protocols, too; XMPP/Jabber is similar but using XML as the self-describing data format.

The type system exposes a lot of very low-level types.  I intend to build more higher-level types on top, but the goal at this stage is to provide all the primitives.  After that, I can play around and stuff.  I don’t plan for the higher-level types to be part of the “standard”, because that wouldn’t buy anything - they don’t need to be agreed upon in advance to use them, since the schema is expressible in the system itself.

For aggregate types, I originally broke it down to array (one-dimensional, nothing fancy with the indices; anything more complicated can be built on top); union, which assigns a name to each field it may contain, and exactly one of those fields is present in an actual value; structure, which also assigns a name to each field, but all the fields are present.  Structures, notably, did not have field orders; that is, two structure types with the same field names and types were equivalent regardless of the order the fields appeared in.  And the last aggregate type is “named”.  Notice that in array, union, and structure, the aggregate doesn’t have its own name, which makes anonymous ones possible.  The capability is separated out.

For text the type is UTF-8.  There are no types for other character sets or encodings, because that’s the right one 99% of the time.

There’s blob, too, so if you need to embed non-Modern data in some known format, you can just make a blob out of it.

However, the type system has been simplified now - instead of array, union, and structure, there’s just sigma.</Text>
        </Document>
    </Documents>
</SearchIndexes>